A1---->

UNIGRAM natural     	: -13.766408817
BIGRAM natural that	: -4.05889368905
TRIGRAM natural that he : -1.58496250072

A2---->

The perplexity is 

A2.uni.txt : 1052.4865859
A2.bi.txt  : 55.6691185628
A2.tri.txt : 5.89828333505


A3---->

A3.txt : 13.934904581

A4---->

A better language model is one which assigns highest probability to the word that actually occurs. 
Perplexity is the probability of test set normalized by the number of words. Minimizing perplexity is maximizng probability. 
Therefore, between language models with and without linear interpolation, the result is approximately as I had expected. 
Trigram has a better model in this case given the similarity between training and testing set.

A5---->

Sample_scored1.txt : 11.4566856621
Sample_scored2.txt : 6.55726714119e+306

B2---->

TRIGRAM CONJ ADV ADP : -2.9755173148
TRIGRAM DET NOUN NUM : -8.9700526163
TRIGRAM NOUN PRT PRON: -11.0854724592

B4---->

* * 	   : 0.0
Night NOUN : -13.8819025994
Place VERB : -15.4538814891
prime ADJ  : -10.6948327183
STOP STOP  : 0.0
_RARE_VERB : -3.17732085089

B6---->

Percent correct tags : 87.998514677 
